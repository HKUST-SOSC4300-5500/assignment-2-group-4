{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e34dd198",
   "metadata": {},
   "source": [
    "# Assignment #2 - SOSC5500 - Group4\n",
    "    \n",
    "**Student: WU Jinfeng, XU Muyao**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "558973c4",
   "metadata": {},
   "source": [
    "## Import Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a3a3f0df",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "pd.options.mode.chained_assignment = None\n",
    "import re\n",
    "import collections\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.ensemble import GradientBoostingClassifier,RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression,RidgeClassifier\n",
    "from sklearn import svm\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import cross_val_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ca6a7cf",
   "metadata": {},
   "source": [
    "## Open the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1b5d6a6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(r\"C:\\Users\\Jinfeng\\OneDrive - HKUST Connect\\Group_Work_SOSC5500\\SOSC5500-Assignments\\Assignment_2 - Due April 4\\Data\") \n",
    "\n",
    "train = pd.read_csv('game_train.csv')\n",
    "test  = pd.read_csv('game_test.csv')\n",
    "\n",
    "X_Train        =  train.drop(columns = ['user_suggestion','year','title'])\n",
    "Y_Train        =  pd.get_dummies(train[['user_suggestion']])\n",
    "\n",
    "X_Predict      =  test.drop(columns = ['year','title'])\n",
    "Predict_ID     =  test['review_id']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "589251b8",
   "metadata": {},
   "source": [
    "## Functions for Word Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "46aace9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "porter = PorterStemmer() \n",
    "\n",
    "def stemSentence(sentence):\n",
    "    token_words=word_tokenize(sentence)\n",
    "    stem_sentence=[]\n",
    "    for word in token_words:\n",
    "        stem_sentence.append(porter.stem(word))\n",
    "        stem_sentence.append(\" \")\n",
    "    return \"\".join(stem_sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dda9d64",
   "metadata": {},
   "source": [
    "## Text Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3369bef0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text_round1(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub('\\w*\\d\\w*', '', text)\n",
    "    text = re.sub('\\n','', text)\n",
    "    text = re.sub(r'[^a-zA-Z^ ]', '',  text)\n",
    "    text = re.sub('  ', ' ',  text)\n",
    "    text = stemSentence(text)\n",
    "\n",
    "    return text\n",
    "\n",
    "round1 = lambda x: clean_text_round1(x)\n",
    "\n",
    "X_Train_clean   = pd.DataFrame(X_Train.user_review.apply(round1))\n",
    "X_Predict_clean = pd.DataFrame(X_Predict.user_review.apply(round1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93ef4a72",
   "metadata": {},
   "source": [
    "## Document-Term Matrix based on TF-IDF Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0169ddf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop = list(stopwords.words('english'))\n",
    "stop.extend(['write','game','would','say','review','year','access','also'])\n",
    "\n",
    "vectorization = TfidfVectorizer(stop_words= stop,ngram_range = (1,4),min_df=3) ## 1-4_gram, and remove less frequent words\n",
    "X_Train_tf    = vectorization.fit_transform(X_Train_clean.user_review)\n",
    "X_Train_tf    = pd.DataFrame(X_Train_tf.toarray(), columns=vectorization.get_feature_names())\n",
    "\n",
    "X_Predict_tf  = vectorization.transform(X_Predict_clean.user_review)\n",
    "X_Predict_tf  = pd.DataFrame(X_Predict_tf.toarray(), columns=vectorization.get_feature_names())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb0f89ea",
   "metadata": {},
   "source": [
    "## Merge the DTM and Game Information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1df2bc86",
   "metadata": {},
   "outputs": [],
   "source": [
    "title = pd.concat([train[['title','review_id']],test[['title','review_id']]],axis=0)\n",
    "title['developer'] = 'NA'\n",
    "title['publisher'] = 'NA'\n",
    "title.index = range(0,17490,1)\n",
    "\n",
    "game_info= pd.read_csv('games.csv') \n",
    "\n",
    "for i in range(len(title)):\n",
    "    title['developer'][i] = list(game_info['developer'][game_info['title'] == title['title'][i]])[0]\n",
    "    title['publisher'][i] = list(game_info['publisher'][game_info['title'] == title['title'][i]])[0]\n",
    "\n",
    "ohe = OneHotEncoder()\n",
    "var_categorical = ['title','developer','publisher']\n",
    "\n",
    "title = pd.DataFrame(ohe.fit_transform(title[var_categorical]).toarray(), columns= ohe.get_feature_names())\n",
    "title_train = title[title.index < 10494]\n",
    "title_test = title[title.index >= 10494]\n",
    "title_test.index = range(0,6996,1)\n",
    "\n",
    "X_Train_all   = pd.concat([X_Train_tf, title_train],axis=1)\n",
    "X_Predict_all = pd.concat([X_Predict_tf, title_test],axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f39e6d4",
   "metadata": {},
   "source": [
    "## Supervised Learning with Different Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f40c5b87",
   "metadata": {},
   "outputs": [],
   "source": [
    "## LASSO\n",
    "model_lasso = LogisticRegression(C=5,penalty='l1', solver='liblinear')\n",
    "scores_lasso = cross_val_score(model_lasso, X_Train_all, Y_Train.values.ravel(), cv=3,scoring='f1_macro')\n",
    "print(scores_lasso) ## [0.85480919 0.84180381 0.84891316]\n",
    "\n",
    "## Ridge\n",
    "model_ridge = RidgeClassifier(alpha=0.8)\n",
    "scores_ridge = cross_val_score(model_ridge, X_Train_all, Y_Train.values.ravel(), cv=3,scoring='f1_macro')\n",
    "print(scores_ridge) ## [0.85906267 0.84457093 0.86414432]\n",
    "\n",
    "## SVM\n",
    "model_svm = svm.SVC()\n",
    "scores_svm = cross_val_score(model_ridge, X_Train_all, Y_Train.values.ravel(), cv=3,scoring='f1_macro')\n",
    "print(scores_svm) ## [0.84682203 0.83850338 0.85110879]\n",
    "\n",
    "## Random Forest\n",
    "model_RandomForest =RandomForestClassifier(n_estimators=200)\n",
    "scores_rf = cross_val_score(model_RandomForest, X_Train_all, Y_Train.values.ravel(), cv=3,scoring='f1_macro')\n",
    "print(scores_rf) ## [0.82727251 0.81669429 0.82981801]\n",
    "\n",
    "## Boosting Trees\n",
    "model_BT = GradientBoostingClassifier()\n",
    "scores_bt = cross_val_score(model_BT, X_Train_all, Y_Train.values.ravel(), cv=3,scoring='f1_macro')\n",
    "print(scores_bt) ## [0.80030397 0.78173007 0.78261874]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "299378ff",
   "metadata": {},
   "source": [
    "## Tune Parameters for Lasso Model and Ridge Model\n",
    "\n",
    "The F-1 scores of Lasso model and Ridge model are relatively better, so we choose to tune parameters for these two models. Results show that when C=5, Lasso model can produce the best result; and when Alpha=0.8, Ridge model can produce the best result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9c50a39",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Tune Parameters for Lasso Model\n",
    "\n",
    "param_grid = {'C': [i for i in range(1,100,2)],\n",
    "              'penalty':['l1'],\n",
    "              'solver':['liblinear']}\n",
    "\n",
    "bt_Grid = GridSearchCV(estimator  = LogisticRegression(), \n",
    "                       param_grid = param_grid, cv = 3, verbose=3, scoring='f1')\n",
    "bt_Grid.fit(X_Train_tf,Y_Train.values.ravel())\n",
    "bt_Grid.best_params_ ## [C=5] \n",
    "\n",
    "## Tune Parameters for Ridge Model\n",
    "\n",
    "param_grid = {'alpha': [0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9]}\n",
    "\n",
    "bt_Grid = GridSearchCV(estimator  = RidgeClassifier(), \n",
    "                       param_grid = param_grid, cv = 3, verbose=3, scoring='f1')\n",
    "bt_Grid.fit(X_Train_tf,Y_Train.values.ravel())\n",
    "bt_Grid.best_params_ ## [Alpha:0.8] "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa6be59c",
   "metadata": {},
   "source": [
    "## Combine the Prediction from Lasso and Ridge Model\n",
    "\n",
    "In this part, we combined the result from different models. Because the F-1 of Lasso, Ridge models are relatively high, we only consider these two models in this part. If a case is identifies as Yes by Lasso Model or Ridge Model, we code it as Yes. Otherwise, we code a case as No.\n",
    "\n",
    "With this method, we got the best prediction result and the F-1 score is 0.86077."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2469b93c",
   "metadata": {},
   "outputs": [],
   "source": [
    "c = []\n",
    "for i in range(len(Y_Predict_Lasso)):\n",
    "    ci = Y_Predict_Lasso[i]+ Y_Predict_ridge[i]\n",
    "    if ci>=1:\n",
    "        c.append(1)\n",
    "    elif ci<1:\n",
    "        c.append(0)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
